{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab953bd6-caa0-45b7-b4af-fb7fc8fb961f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from mtcnn.mtcnn import MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c60ba6f6-5286-4b89-bc80-ef0d5f08f1e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detector = MTCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "467d4184-f2c5-424a-bcf5-838fbe68a3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "IMAGENET_STD  = np.array([0.229, 0.224, 0.225], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56fe7e41-6a35-4106-bf1c-fa06ea29505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_crop(frame, size=224):\n",
    "    try:\n",
    "        h, w, _ = frame.shape\n",
    "        faces = detector.detect_faces(frame)\n",
    "\n",
    "        if faces:\n",
    "            best = max(faces, key=lambda d: d.get('confidence', 0.0))\n",
    "            x, y, fw, fh = best['box']\n",
    "            x = max(0, x); y = max(0, y)\n",
    "            x2 = min(w, x + fw); y2 = min(h, y + fh)\n",
    "            face = frame[y:y2, x:x2]\n",
    "        else:\n",
    "            # fallback: center crop\n",
    "            crop_side = int(min(h, w) * 0.6)\n",
    "            cx, cy = w // 2, h // 2\n",
    "            x = cx - crop_side // 2\n",
    "            y = cy - crop_side // 2\n",
    "            face = frame[y:y + crop_side, x:x + crop_side]\n",
    "\n",
    "        face = cv2.resize(face, (size, size))\n",
    "        face = face.astype(np.float32) / 255.0\n",
    "        face = (face - IMAGENET_MEAN) / IMAGENET_STD\n",
    "        return face\n",
    "    except Exception as e:\n",
    "        print(\"Error in face_crop:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2dfc46f8-ef0d-4c29-8811-8cac8c63d9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(root_dir, batch_size=16, img_size=224, is_training=True):\n",
    "    \n",
    "    real_dir = os.path.join(root_dir, \"original\")\n",
    "    fake_dirs = [\n",
    "        os.path.join(root_dir, d)\n",
    "        for d in [\"Deepfakes\", \"Face2Face\", \"FaceSwap\", \"NeuralTextures\"]\n",
    "        if os.path.exists(os.path.join(root_dir, d))\n",
    "    ]\n",
    "\n",
    "    real_files = [os.path.join(real_dir, f) for f in os.listdir(real_dir) if f.lower().endswith((\".jpg\", \".png\"))]\n",
    "    fake_files = []\n",
    "    \n",
    "    for fd in fake_dirs:\n",
    "        fake_files.extend([os.path.join(fd, f) for f in os.listdir(fd) if f.lower().endswith((\".jpg\", \".png\"))])\n",
    "\n",
    "    split_real = int(0.8 * len(real_files))\n",
    "    split_fake = int(0.8 * len(fake_files))\n",
    "\n",
    "    if is_training:\n",
    "        real_list = real_files[:split_real]\n",
    "        fake_list = fake_files[:split_fake]\n",
    "    else:\n",
    "        real_list = real_files[split_real:]\n",
    "        fake_list = fake_files[split_fake:]\n",
    "\n",
    "    print(f\" Real: {len(real_list)}  |  Fake: {len(fake_list)}\")\n",
    "\n",
    "    while True:\n",
    "        half = batch_size // 2\n",
    "\n",
    "        real_batch = random.sample(real_list, half)\n",
    "        fake_batch = random.sample(fake_list, half)\n",
    "\n",
    "        batch_files = real_batch + fake_batch\n",
    "        batch_labels = [0]*half + [1]*half\n",
    "\n",
    "        combined = list(zip(batch_files, batch_labels))\n",
    "        random.shuffle(combined)\n",
    "\n",
    "        batch_x, batch_y = [], []\n",
    "        for path, label in combined:\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            face = face_crop(img, size=img_size)\n",
    "            if face is not None:\n",
    "                batch_x.append(face)\n",
    "                batch_y.append(label)\n",
    "\n",
    "        batch_x = np.array(batch_x)\n",
    "        batch_y = tf.keras.utils.to_categorical(batch_y, num_classes=2)\n",
    "\n",
    "        yield batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2de9a156-3cae-453f-b11f-c3962508b1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original : 15 files\n",
      "Deepfakes : 14 files\n",
      "Face2Face : 12 files\n",
      "FaceSwap : 12 files\n",
      "NeuralTextures : 14 files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root_dir = \"FF++\"\n",
    "for cls in [\"original\", \"Deepfakes\", \"Face2Face\", \"FaceSwap\", \"NeuralTextures\"]:\n",
    "    path = os.path.join(root_dir, cls)\n",
    "    if os.path.exists(path):\n",
    "        print(cls, \":\", len(os.listdir(path)), \"files\")\n",
    "    else:\n",
    "        print(cls, \"folder not found \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bebf9ded-7bbe-4f01-9eb4-3901deecf02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = data_generator(\"FF++\", batch_size=32, is_training=True)\n",
    "val_gen   = data_generator(\"FF++\", batch_size=32, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "277e3f41-a84e-449b-bcf9-7ca4537cd348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Real: 12  |  Fake: 41\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m train_gen \u001b[38;5;241m=\u001b[39m data_generator(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFF++\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m val_gen   \u001b[38;5;241m=\u001b[39m data_generator(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFF++\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape, y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReal/Fake ratio:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39msum(y[:,\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39msum(y[:,\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[1;32mIn[25], line 31\u001b[0m, in \u001b[0;36mdata_generator\u001b[1;34m(root_dir, batch_size, img_size, is_training)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m     half \u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m---> 31\u001b[0m     real_batch \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhalf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     fake_batch \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(fake_list, half)\n\u001b[0;32m     34\u001b[0m     batch_files \u001b[38;5;241m=\u001b[39m real_batch \u001b[38;5;241m+\u001b[39m fake_batch\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\random.py:482\u001b[0m, in \u001b[0;36mRandom.sample\u001b[1;34m(self, population, k, counts)\u001b[0m\n\u001b[0;32m    480\u001b[0m randbelow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_randbelow\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m k \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n:\n\u001b[1;32m--> 482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample larger than population or is negative\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    483\u001b[0m result \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m k\n\u001b[0;32m    484\u001b[0m setsize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m21\u001b[39m        \u001b[38;5;66;03m# size of a small set minus size of an empty list\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "train_gen = data_generator(\"FF++\", batch_size=32, is_training=True)\n",
    "val_gen   = data_generator(\"FF++\", batch_size=32, is_training=False)\n",
    "\n",
    "X, y = next(train_gen)\n",
    "print(\"Batch shape:\", X.shape, y.shape)\n",
    "print(\"Real/Fake ratio:\", np.sum(y[:,1]==0), \"/\", np.sum(y[:,1]==1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b713cfee-417f-4b8c-8f4a-d9fd7d13e397",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = min(len(real_list), len(fake_list)) // (batch_size // 2)\n",
    "validation_steps=steps_per_epoch // 4, \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
